{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai \n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and model name from environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_model = os.getenv(\"OPENAI_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with a valid model name\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # Use a valid OpenAI model name\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Temp\\ipykernel_13504\\800757705.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings()\n",
      "C:\\Users\\karti\\AppData\\Local\\Temp\\ipykernel_13504\\800757705.py:5: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory = persist_directory, embedding_function=embedding)\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "persist_directory = r'C:\\Users\\karti\\Desktop\\Project\\openai_learning\\vector_database\\Learning'\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory = persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the major thing discuss in the pdf\"\n",
    "docs = vectordb.similarity_search(question , k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Vision–ECCV 2014, pages 299–314. Springer, 2014. 7\n",
      "[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\n",
      "object detection using deep neural networks. In Computer\n",
      "Vision and Pattern Recognition (CVPR), 2014 IEEE Confer-\n",
      "ence on, pages 2155–2162. IEEE, 2014. 5, 6\n",
      "[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\n",
      "Williams, J. Winn, and A. Zisserman. The pascal visual ob-\n",
      "ject classes challenge: A retrospective.International Journal\n",
      "of Computer Vision, 111(1):98–136, Jan. 2015. 2\n",
      "[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\n",
      "manan. Object detection with discriminatively trained part\n",
      "based models. IEEE Transactions on Pattern Analysis and\n",
      "Machine Intelligence, 32(9):1627–1645, 2010. 1, 4\n",
      "[11] S. Gidaris and N. Komodakis. Object detection via a multi-\n",
      "region & semantic segmentation-aware CNN model. CoRR,\n",
      "abs/1505.01749, 2015. 7\n",
      "[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\n",
      "ple in cubist art. InComputer Vision-ECCV 2014 Workshops,\n",
      "pages 101–116. Springer, 2014. 7\n",
      "[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\n",
      "ture hierarchies for accurate object detection and semantic\n",
      "segmentation. In Computer Vision and Pattern Recognition\n",
      "(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\n",
      "2014. 1, 4, 7\n",
      "[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\n",
      "2, 5, 6, 7\n",
      "[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\n",
      "tion and object detection. In Advances in neural information' metadata={'page': 8, 'page_label': '9', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}\n",
      "page_content='Vision–ECCV 2014, pages 299–314. Springer, 2014. 7\n",
      "[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\n",
      "object detection using deep neural networks. In Computer\n",
      "Vision and Pattern Recognition (CVPR), 2014 IEEE Confer-\n",
      "ence on, pages 2155–2162. IEEE, 2014. 5, 6\n",
      "[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\n",
      "Williams, J. Winn, and A. Zisserman. The pascal visual ob-\n",
      "ject classes challenge: A retrospective.International Journal\n",
      "of Computer Vision, 111(1):98–136, Jan. 2015. 2\n",
      "[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\n",
      "manan. Object detection with discriminatively trained part\n",
      "based models. IEEE Transactions on Pattern Analysis and\n",
      "Machine Intelligence, 32(9):1627–1645, 2010. 1, 4\n",
      "[11] S. Gidaris and N. Komodakis. Object detection via a multi-\n",
      "region & semantic segmentation-aware CNN model. CoRR,\n",
      "abs/1505.01749, 2015. 7\n",
      "[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\n",
      "ple in cubist art. InComputer Vision-ECCV 2014 Workshops,\n",
      "pages 101–116. Springer, 2014. 7\n",
      "[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\n",
      "ture hierarchies for accurate object detection and semantic\n",
      "segmentation. In Computer Vision and Pattern Recognition\n",
      "(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\n",
      "2014. 1, 4, 7\n",
      "[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\n",
      "2, 5, 6, 7\n",
      "[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\n",
      "tion and object detection. In Advances in neural information' metadata={'page': 8, 'page_label': '9', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}\n",
      "page_content='Vision–ECCV 2014, pages 299–314. Springer, 2014. 7\n",
      "[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\n",
      "object detection using deep neural networks. In Computer\n",
      "Vision and Pattern Recognition (CVPR), 2014 IEEE Confer-\n",
      "ence on, pages 2155–2162. IEEE, 2014. 5, 6\n",
      "[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\n",
      "Williams, J. Winn, and A. Zisserman. The pascal visual ob-\n",
      "ject classes challenge: A retrospective.International Journal\n",
      "of Computer Vision, 111(1):98–136, Jan. 2015. 2\n",
      "[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\n",
      "manan. Object detection with discriminatively trained part\n",
      "based models. IEEE Transactions on Pattern Analysis and\n",
      "Machine Intelligence, 32(9):1627–1645, 2010. 1, 4\n",
      "[11] S. Gidaris and N. Komodakis. Object detection via a multi-\n",
      "region & semantic segmentation-aware CNN model. CoRR,\n",
      "abs/1505.01749, 2015. 7\n",
      "[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\n",
      "ple in cubist art. InComputer Vision-ECCV 2014 Workshops,\n",
      "pages 101–116. Springer, 2014. 7\n",
      "[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\n",
      "ture hierarchies for accurate object detection and semantic\n",
      "segmentation. In Computer Vision and Pattern Recognition\n",
      "(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\n",
      "2014. 1, 4, 7\n",
      "[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\n",
      "2, 5, 6, 7\n",
      "[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\n",
      "tion and object detection. In Advances in neural information' metadata={'page': 8, 'page_label': '9', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}\n",
      "page_content='5 C ONCLUSIONS\n",
      "In this work we introduced theTriplet network model, a tool that uses a deep network to learn useful\n",
      "representation explicitly. The results shown on various datasets provide evidence that the represen-\n",
      "tations that were learned are useful to classiﬁcation in a way that is comparable with a network that\n",
      "was trained explicitly to classify samples. We believe that enhancement to the embedding network\n",
      "such as Network-in-Network model (Lin et al. (2013)), Inception models (Szegedy et al. (2014))\n",
      "and others can beneﬁt the Triplet net similarly to the way they beneﬁted other classiﬁcation tasks.\n",
      "Considering the fact that this method requires to know only that two out of three images are sampled\n",
      "from the same class, rather than knowing what that class is, we think this should be inquired further,\n",
      "and may provide us insights to the way deep networks learn in general. We have also shown how\n",
      "this model learns using only comparative measures instead of labels, which we can use in the future\n",
      "to leverage new data sources for which clear out labels are not known or do not make sense (e.g\n",
      "hierarchical labels).\n",
      "6' metadata={'page': 5, 'page_label': '6', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\Triplet_loss_original_paper.pdf'}\n",
      "page_content='5 C ONCLUSIONS\n",
      "In this work we introduced theTriplet network model, a tool that uses a deep network to learn useful\n",
      "representation explicitly. The results shown on various datasets provide evidence that the represen-\n",
      "tations that were learned are useful to classiﬁcation in a way that is comparable with a network that\n",
      "was trained explicitly to classify samples. We believe that enhancement to the embedding network\n",
      "such as Network-in-Network model (Lin et al. (2013)), Inception models (Szegedy et al. (2014))\n",
      "and others can beneﬁt the Triplet net similarly to the way they beneﬁted other classiﬁcation tasks.\n",
      "Considering the fact that this method requires to know only that two out of three images are sampled\n",
      "from the same class, rather than knowing what that class is, we think this should be inquired further,\n",
      "and may provide us insights to the way deep networks learn in general. We have also shown how\n",
      "this model learns using only comparative measures instead of labels, which we can use in the future\n",
      "to leverage new data sources for which clear out labels are not known or do not make sense (e.g\n",
      "hierarchical labels).\n",
      "6' metadata={'page': 5, 'page_label': '6', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\Triplet_loss_original_paper.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever = vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the pdf about\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Temp\\ipykernel_13504\\3836497603.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = qa_chain.retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: [Document(metadata={'page': 0, 'page_label': '1', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}, page_content='means we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/.\\nSecond, YOLO reasons globally about the image when\\n1\\narXiv:1506.02640v5  [cs.CV]  9 May 2016'), Document(metadata={'page': 0, 'page_label': '1', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}, page_content='means we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/.\\nSecond, YOLO reasons globally about the image when\\n1\\narXiv:1506.02640v5  [cs.CV]  9 May 2016'), Document(metadata={'page': 0, 'page_label': '1', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}, page_content='means we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/.\\nSecond, YOLO reasons globally about the image when\\n1\\narXiv:1506.02640v5  [cs.CV]  9 May 2016'), Document(metadata={'page': 8, 'page_label': '9', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\yolo.pdf'}, page_content='Vision–ECCV 2014, pages 299–314. Springer, 2014. 7\\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\\nobject detection using deep neural networks. In Computer\\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-\\nence on, pages 2155–2162. IEEE, 2014. 5, 6\\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.\\nWilliams, J. Winn, and A. Zisserman. The pascal visual ob-\\nject classes challenge: A retrospective.International Journal\\nof Computer Vision, 111(1):98–136, Jan. 2015. 2\\n[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\\nmanan. Object detection with discriminatively trained part\\nbased models. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 32(9):1627–1645, 2010. 1, 4\\n[11] S. Gidaris and N. Komodakis. Object detection via a multi-\\nregion & semantic segmentation-aware CNN model. CoRR,\\nabs/1505.01749, 2015. 7\\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-\\nple in cubist art. InComputer Vision-ECCV 2014 Workshops,\\npages 101–116. Springer, 2014. 7\\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. In Computer Vision and Pattern Recognition\\n(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\\n2014. 1, 4, 7\\n[14] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015.\\n2, 5, 6, 7\\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-\\ntion and object detection. In Advances in neural information')]\n"
     ]
    }
   ],
   "source": [
    "docs = qa_chain.retriever.get_relevant_documents(question)\n",
    "print(\"Retrieved Documents:\", docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Temp\\ipykernel_13504\\1247687008.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = qa_chain({\"query\": question})\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'what is the pdf about',\n",
       " 'result': \"I don't have enough context to determine what specific PDF you are referring to. If you can provide more details or a specific title, I may be able to help you better.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # need to specify parameter name\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,  # correct parameter name\n",
    "    chain_type=\"stuff\",  # specify chain type\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}  # correct dictionary syntax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"what is simense network\" \n",
    "result = qa_chain({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Siamese networks are a type of neural network architecture that are sensitive to calibration and context when determining similarity between objects or individuals. They are often used for tasks such as image recognition and classification. Thanks for asking!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'page_label': '1', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\Triplet_loss_original_paper.pdf'}, page_content='Siamese networks are also sensitive to calibration in the sense that the notion of similarity vs dis-\\nsimilarity requires context. For example, a person might be deemed similar to another person when\\na dataset of random objects is provided, but might be deemed dissimilar with respect to the same\\nother person when we wish to distinguish between two individuals in a set of individuals only. In\\nour model, such a calibration is not required. In fact, in our experiments here, we have experienced\\nhands on the difﬁculty in using Siamese networks.\\nWe follow a similar task to that of Chechik et al. (2010). For a set of samples P and a chosen rough\\nsimilarity measure r(x, x′) given through a training oracle (e.g how close are two images of objects\\n∗The author acknowledges the generous support of ISF grant number 1271/13\\n1\\narXiv:1412.6622v4  [cs.LG]  4 Dec 2018'),\n",
       " Document(metadata={'page': 0, 'page_label': '1', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\Triplet_loss_original_paper.pdf'}, page_content='Siamese networks are also sensitive to calibration in the sense that the notion of similarity vs dis-\\nsimilarity requires context. For example, a person might be deemed similar to another person when\\na dataset of random objects is provided, but might be deemed dissimilar with respect to the same\\nother person when we wish to distinguish between two individuals in a set of individuals only. In\\nour model, such a calibration is not required. In fact, in our experiments here, we have experienced\\nhands on the difﬁculty in using Siamese networks.\\nWe follow a similar task to that of Chechik et al. (2010). For a set of samples P and a chosen rough\\nsimilarity measure r(x, x′) given through a training oracle (e.g how close are two images of objects\\n∗The author acknowledges the generous support of ISF grant number 1271/13\\n1\\narXiv:1412.6622v4  [cs.LG]  4 Dec 2018'),\n",
       " Document(metadata={'page': 0, 'page_label': '1', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\Triplet_loss_original_paper.pdf'}, page_content='Siamese networks are also sensitive to calibration in the sense that the notion of similarity vs dis-\\nsimilarity requires context. For example, a person might be deemed similar to another person when\\na dataset of random objects is provided, but might be deemed dissimilar with respect to the same\\nother person when we wish to distinguish between two individuals in a set of individuals only. In\\nour model, such a calibration is not required. In fact, in our experiments here, we have experienced\\nhands on the difﬁculty in using Siamese networks.\\nWe follow a similar task to that of Chechik et al. (2010). For a set of samples P and a chosen rough\\nsimilarity measure r(x, x′) given through a training oracle (e.g how close are two images of objects\\n∗The author acknowledges the generous support of ISF grant number 1271/13\\n1\\narXiv:1412.6622v4  [cs.LG]  4 Dec 2018'),\n",
       " Document(metadata={'page': 3, 'page_label': '4', 'source': 'C:\\\\Users\\\\karti\\\\Desktop\\\\Project\\\\openai_learning\\\\docs\\\\Triplet_loss_original_paper.pdf'}, page_content='their embedding and, as shown in the results, can reach high classiﬁcation accuracy using a simple\\nsubsequent linear classiﬁer.\\n3.5 C OMPARISON WITH PERFORMANCE OF THE SIAMESE NETWORK\\nThe Siamese network is the most obvious competitor for our approach. Our implementation of the\\nSiamese network consisted of the same embedding network, but with the use of a contrastive loss\\nbetween a pair of samples, instead of three (as explained in Chopra et al. (2005)). The generated\\nfeatures were then used for classiﬁcation using a similar linear model as was used for the TripletNet\\nmethod. We measured lower accuracy on the MNIST dataset compared to results gained using the\\nTripletNet representations 2.\\nWe have tried a similar comparison for the other three datasets, but unfortunately could not obtain\\nany meaningful result using a Siamese network. We conjecture that this might be related to the\\nproblem of context described above, and leave the resolution of this conjecture to future work.\\n4')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"is math important\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # need to specify parameter name\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,  # correct parameter name\n",
    "    chain_type=\"map_reduce\",  # specify chain type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain({\"query\" : query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Siamese network is a type of neural network architecture that contains two identical subnetworks which are joined at the output layer. This architecture is commonly used for tasks involving similarity or distance measurement, such as image recognition, signature verification, and more. The network is trained to learn how to differentiate between similar and dissimilar inputs by minimizing the distance between similar pairs and maximizing the distance between dissimilar pairs in the output space.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.3'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.6.0/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.3.min.js\", \"https://cdn.holoviz.org/panel/1.6.0/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='11cdfde7-2dc1-4290-967d-dbdd332d3610'>\n",
       "  <div id=\"ec75599a-a076-4a60-bbda-1a22d4ce1241\" data-root-id=\"11cdfde7-2dc1-4290-967d-dbdd332d3610\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"8d1e10b5-d251-4fef-aa0d-e5afb5ffc3b3\":{\"version\":\"3.6.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"11cdfde7-2dc1-4290-967d-dbdd332d3610\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"a904529b-67ae-42c9-81b9-b088dbdd9070\",\"attributes\":{\"plot_id\":\"11cdfde7-2dc1-4290-967d-dbdd332d3610\",\"comm_id\":\"ece465c1b84548bcbd7cb49a44271ea6\",\"client_comm_id\":\"029c97cd7d9941fc9fc9c51dac64359d\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"8d1e10b5-d251-4fef-aa0d-e5afb5ffc3b3\",\"roots\":{\"11cdfde7-2dc1-4290-967d-dbdd332d3610\":\"ec75599a-a076-4a60-bbda-1a22d4ce1241\"},\"root_ids\":[\"11cdfde7-2dc1-4290-967d-dbdd332d3610\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "11cdfde7-2dc1-4290-967d-dbdd332d3610"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import panel as pn \n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Temp\\ipykernel_13504\\1160254780.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages= True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # need to specify parameter name\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    memory = memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the pdf about\"\n",
    "result = qa_chain({'question' : question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what are the pdf about',\n",
       " 'chat_history': [HumanMessage(content='what are the pdf about', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I don't know.\", additional_kwargs={}, response_metadata={})],\n",
       " 'answer': \"I don't know.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA,  ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0629f22e480c4d16b78b70eaa7960508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'357d5d31-b7e7-4eec-aef1-89310b874cca': {'version…"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to load the database\n",
    "def load_db(file, chain_type, k):\n",
    "    # Load documents\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Define embedding\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    # Create vector database from data\n",
    "    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
    "    \n",
    "    # Define retriever\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    \n",
    "    # Create a chatbot chain\n",
    "    qa = ConversationalRetrievalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "        chain_type=chain_type,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        return_generated_question=True,\n",
    "    )\n",
    "    return qa\n",
    "\n",
    "# ChatBot class\n",
    "class ChatBot(param.Parameterized):\n",
    "    chat_history = param.List([])\n",
    "    answer = param.String(\"\")\n",
    "    db_query = param.String(\"\")\n",
    "    db_response = param.List([])\n",
    "    \n",
    "    def __init__(self, **params):\n",
    "        super().__init__(**params)\n",
    "        self.panels = []\n",
    "        self.loaded_file = r\"C:\\Users\\karti\\Desktop\\Project\\openai_learning\\docs\\Triplet_loss_original_paper.pdf\"\n",
    "        self.qa = load_db(self.loaded_file, \"stuff\", 4)\n",
    "    \n",
    "    def call_load_db(self, count):\n",
    "        if count == 0 or file_input.value is None:\n",
    "            return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "        \n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(file_input.value)\n",
    "        \n",
    "        self.loaded_file = file_input.filename\n",
    "        button_load.button_type = \"primary\"\n",
    "        self.qa = load_db(\"temp.pdf\", \"stuff\", 4)\n",
    "        button_load.button_type = \"success\"\n",
    "        self.clr_history()\n",
    "        return pn.pane.Markdown(f\"Loaded File: {self.loaded_file}\")\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n",
    "        \n",
    "        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.append((query, result[\"answer\"]))\n",
    "        self.db_query = result.get(\"generated_question\", \"\")\n",
    "        self.db_response = result.get(\"source_documents\", [])\n",
    "        self.answer = result['answer']\n",
    "        \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, styles={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        \n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "\n",
    "    @param.depends('db_query')\n",
    "    def get_lquest(self):\n",
    "        return pn.Column(\n",
    "            pn.pane.Markdown(\"Last query to DB:\" if self.db_query else \"No DB accesses yet\", styles={'background-color': '#F6F6F6'}),\n",
    "            pn.pane.Str(self.db_query or \"No DB accesses so far\")\n",
    "        )\n",
    "    \n",
    "    @param.depends('db_response')\n",
    "    def get_sources(self):\n",
    "        if not self.db_response:\n",
    "            return pn.pane.Markdown(\"No sources yet\")\n",
    "        \n",
    "        return pn.WidgetBox(\n",
    "            pn.pane.Markdown(\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}),\n",
    "            *[pn.pane.Str(doc) for doc in self.db_response],\n",
    "            width=600, scroll=True\n",
    "        )\n",
    "\n",
    "    @param.depends('chat_history')\n",
    "    def get_chats(self):\n",
    "        if not self.chat_history:\n",
    "            return pn.WidgetBox(pn.pane.Str(\"No History Yet\"), width=600, scroll=True)\n",
    "        \n",
    "        return pn.WidgetBox(\n",
    "            pn.pane.Markdown(\"Current Chat History\", styles={'background-color': '#F6F6F6'}),\n",
    "            *[pn.pane.Str(str(exchange)) for exchange in self.chat_history],\n",
    "            width=600, scroll=True\n",
    "        )\n",
    "    \n",
    "    def clr_history(self, event=None):\n",
    "        self.chat_history = []\n",
    "        self.panels = []\n",
    "\n",
    "# Initialize components\n",
    "chatbot = ChatBot()\n",
    "file_input = pn.widgets.FileInput(accept='.pdf')\n",
    "button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\n",
    "button_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\n",
    "button_clearhistory.on_click(chatbot.clr_history)\n",
    "inp = pn.widgets.TextInput(placeholder='Enter text here…')\n",
    "\n",
    "# Bind button and conversation\n",
    "bound_button_load = pn.bind(chatbot.call_load_db, button_load.param.clicks)\n",
    "conversation = pn.bind(chatbot.convchain, inp)\n",
    "\n",
    "# Create tabs\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation, loading_indicator=True, height=300),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "tab2 = pn.Column(\n",
    "    pn.panel(chatbot.get_lquest),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(chatbot.get_sources),\n",
    ")\n",
    "\n",
    "tab3 = pn.Column(\n",
    "    pn.panel(chatbot.get_chats),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "tab4 = pn.Column(\n",
    "    pn.Row(file_input, button_load, bound_button_load),\n",
    "    pn.Row(button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\")),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "# Create dashboard\n",
    "dashbaord = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n",
    "    pn.Tabs(\n",
    "        ('Conversation', tab1),\n",
    "        ('Database', tab2),\n",
    "        ('Chat History', tab3),\n",
    "        ('Configure', tab4)\n",
    "    )\n",
    ")\n",
    "\n",
    "dashbaord.servable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
